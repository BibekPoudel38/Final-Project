services:
  # ==========================================
  # NGINX (Reverse Proxy / Gateway)
  # ==========================================
  nginx:
    image: nginx:alpine
    container_name: final_project_nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf
      - static_volume:/app/static
      - media_volume:/app/media
    depends_on:
      - backend
      - frontend
      - prediction
      - ollama
      - llm
    networks:
      - app_network
    restart: always

  # ==========================================
  # BACKEND (Django)
  # ==========================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: final_project_backend
    command: gunicorn bizai.wsgi:application --bind 0.0.0.0:8000
    volumes:
      - ./backend:/app
      - static_volume:/app/static
      - media_volume:/app/media
    env_file:
      - ./backend/bizai/.env
    environment:
      - DEBUG=0
      - ALLOWED_HOSTS=backend,localhost,127.0.0.1,nginx
    networks:
      - app_network
    restart: always

  # ==========================================
  # FRONTEND (React)
  # ==========================================
  frontend:
    build:
      context: ./frontend/BizAI
      dockerfile: Dockerfile
    container_name: final_project_frontend
    # In production, we might serve static files via Nginx, but for "running apps"
    # we'll keep the dev server or a serve command.
    # Using 'npm run dev' with host binding for now to keep it simple and consistent.
    command: npm run dev -- --host
    volumes:
      - ./frontend/BizAI:/app
      - /app/node_modules
    networks:
      - app_network
    restart: always

  # ==========================================
  # PREDICTION MODEL (Flask)
  # ==========================================
  prediction:
    build:
      context: ./prediction_model
      dockerfile: Dockerfile
    container_name: final_project_prediction
    command: python app.py
    volumes:
      - ./prediction_model:/app
      - ./prediction_model/model_store:/app/model_store
    networks:
      - app_network
    restart: always

  # ==========================================
  # LLM APP (Flask + LangChain)
  # ==========================================
  llm:
    build:
      context: ./llm/project
      dockerfile: Dockerfile
    container_name: final_project_llm
    ports:
      - "5000:5000"
    volumes:
      - ./llm/project:/app
    environment:
      - FLASK_PORT=5000
      - GRAPHQL_ENDPOINT=http://backend:8000/graphql/
      - OLLAMA_MODEL=llama3.1:8b
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - backend
      - ollama
    networks:
      - app_network
    restart: always

  # ==========================================
  # LLM (Ollama)
  # ==========================================
  ollama:
    image: ollama/ollama:latest
    container_name: final_project_ollama
    volumes:
      - ollama_volume:/root/.ollama
    networks:
      - app_network
    restart: always
    # GPU Support Configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  static_volume:
  media_volume:
  ollama_volume:

networks:
  app_network:
    driver: bridge
